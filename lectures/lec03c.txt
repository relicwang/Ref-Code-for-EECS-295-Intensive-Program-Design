An important motivation for algorithm/data-structure design is
performance. But before we can talk about the relative performance of
various algorithms and data structures, we need some way to measure
performance. There are two main concerns:

- being hardware-independent

- capturing the behavior of functions

Instead of counting seconds, we'll count "step"s that the algorithm
takes. Each step will be some fixed amount of time, but we won't worry
about how much time each step takes, except to insist that they are
all some fixed amount of time. This seems a bit dubious, since
different steps might take different amounts of time, but we'll return
to this point later in the lecture. For now, a step is a step is a
step.

To capture a function's performance, we need to measure more than just
the time of a complete program. What we'll do is measure the time a
function takes to run based on the size of its input. With a modern
computer, when the input is small, pretty much every function is
fast. What matters is how things change as the input gets larger and
larger. Does the time grow really fast? Or does the time grow slowly
as the input grows?

Lets take an example. Here's the sum function we had from earlier:

;; sum : list-of-numbers -> number
(define (sum alon)
  (cond
    [(empty? l) 0]
    [(cons? l)
     (+ (first l)
        (sum (rest l)))]))

Clearly, if the input is the empty list, (the smallest input), then it
takes the time that the cond takes, and it takes the time that the
empty? predicate takes. Lets call that 2 units. What if we pass in a
one element list?  Well, it takes the time for the cond and the empty
test like before, but we also have to do the cons? test and then the
`first` operation, and then the `rest` operation, and then the time
for `sum` of the empty list. Lets say that all the new stuff is 5
units of time and then add in the 2 units for the empty list. How
about a two element list? 5+5+2.  Three elements? 5+5+5+2. So, for n
elements it takes 5*n+2 steps.

Okay, lets write another function that reverses the elements of a
list:

;; rev : list-of-number -> list-of-number
(check-expect (rev '()) '())
(check-expect (rev (cons 1 '())) (cons 1 '()))
(check-expect (rev (cons 1 (cons 2 '()))) (cons 2 (cons 1 '())))

(define (rev l)
  (cond
    [(empty? l) '()]
    [else (add-at-end (first l) (rev (rest l)))]))

;; add-at-end : number list-of-number -> list-of-number
(check-expect (add-at-end 1 '()) (cons 1 '()))
(check-expect (add-at-end 1 (cons 2 '())) (cons 2 (cons 1 '())))
(define (add-at-end n l)
  (cond
    [(empty? l) (cons n '())]
    [else (cons (first l) (add-at-end n (rest l)))]))

How much time does this reversal function take?

Well, first figure out add-at-end. Looking at the cases and reasoning
as before might conclude that it also takes 4*n+2 steps since it does
nearly the same things, except it doesn't have the `cons?` steps.

How about rev? Well, the empty list case is still about 2 steps.  But
now we have to take add-at-end's time into account. It isn't just some
fixed number of steps, but depends on its second input!

So when we supply a list with one element to rev, we're calling
add-at-end with 0 elements. And with a 2 element list passed to rev,
we're calling add-at-end with 1 element. So we have a formula like
this:

    4*(add-at-end-time (- n 1)) + 2

But this happens at each step! So if we have a 5 element list, we are
going to make 5 calls to add-at-end and end up with something like
this:

   4*a-time(4)+
   4*a-time(3)+
   4*a-time(2)+
   4*a-time(1)+
   4*a-time(0)+
   10

which is the same as this:

     n-1
    ----
10+ \     add-at-end-time(i)      
    /
    ----
     i=0

which is the same as this:

     n-1
    ----
10+ \     4*i+2
    /
    ----
     i=0

and then using, you know, math, we know it is the same as this:

     n-1         n-1
    ----        ----
10+ \     4*i + \    2
    /           /
    ----        ----
     i=0         i=0


and after some more simplification steps, 

  2*n^2 + 10

Okay, so we have three functions we've looked at so far:

 sum --> 5*n+2
 add-at-end --> 4*n+2
 rev --> 2*n^2 + 10

For the purposes of analyzing the running time, we really want to
think of the first two functions as having the same running time.

Why? Well, if we run add-at-end on a computer that's 5 years old it
will probably go slower than sum on a computer today. And we really
don't want to have to analyze algorithms on a per-computer basis! If
we do that, then we have to redo the analysis for each new computer.

So the idea is to try to think if can abstract this away. And yes: we
can. The way this is done is to think about something called "Big
Oh". It captures how functions perform "in the limit" or
"asymptotically".

So, intuitively what's going on is that sum and add-at-end both grow
proportionally to the input, but rev grows with the square of the
input size. So if have a fixed-speed computer, you can always find an
input size such that all bigger inputs are going to be slower with rev
than sum.

So we want to collapse all functions with the same running time into a
certain class that represents all of the ones where varying the speed
of the computer doesn't matter. So, we write:

 O(n)   -- set of all functions that grow linearly (or faster)
 O(n^2) -- set of all functions that grow quadratically (or faster)

And, in general:

 O(f) -- set of all functions that grow with the same
         speed as `f` or faster

Here's the mathematically precise definition:

; O(f) = {g | exists n0, m: for all n > n0, g(n) <= m*f(n) }

What is this saying intuitively? Think of "n0" as indicating where the
input got big enough and "m" as that newer computer that speeds things
up. Lets check: is 5*n+2 in O(n)? Yes; take n0 = 0 and m = 10

  for n > 0, 5*n+2 <= 10*n

well, at n=1, we have 7 <= 10. And n=2 we are going to add 5 to the
left and 10 to the right. And it continues like that.

We can do a similar thing to show that 2*n^2 + 10 is in O(n^2). A
little bit more fiddly is to show that 2*n^2 + 10 is NOT in O(n). It
isn't, but lets just show that n^2 is not in O(n), since it is
smaller.

Assume we had n0 and m such that

  for all n > n0,
     n^2 <= c*n

so that should be true of the number max(c,n)+1 because it is true of
all numbers. Lets call max(c,n)=X.

   (X+1)*(X+1) <= c*(X+1)
   X^2+2*X+1 <= c*X+c
   X^2+2*X <= c*X+(c-1)


expand out X:

   max(c,n)*max(c,n) + 2*max(c,n) <= c*max(c,n)+(c-1)

Okay, so we know that c and n must be related to each other by
>=. Lets first assume that c >= n and thus we can simplify the uses of
max

     c*c + 2*c <= c*c+(c-1)
   => c <= -1

which tells us that c <= -1, but
we know that n is at least 1 and we know that c >= n, so c must greater than 0.

OKay, so it must be the case that n > c. Simplify again:

   n*n+2*n <= c*n+(c-1)
 => n*n+(2-c)*n-(c-1)<=0

But this can't be either. Since this parabola is facing up, with the condition
for all n > n0, this inequation can't possibly be true. That is, when n is greater than
the quadratic formula root (if exists), the quadratic formula (n*n+(2-c)*n-(c-1)) must greater than 0.
Otherwise, if the formula root does not exist, the quadratic formula is always greater than 0.

  Contradiction.

Alright, we can work a lot of things out from first principles here,
but in general it is the case that

  O(n^i) is NOT in O(n^(i+1))

And, in fact, we can ignore any of the terms in a polynomial except
the first one (the one with highest degree) and we can ignore any
constant factors, and we will tell you about more relationships as they
come up.

Lets look at the running time of one more function. But first, we should
design it. It needs a slightly different data definition:

  ;; an NE-list-of-numbers is either:
  ;;  - (cons number '())
  ;;  - (cons number NE-list-of-numbers)

  ;; biggest : ne-list-of-numbers -> number
  ;; ASSUME: all of the numbers are bigger or equal to than 0
  (check-expect (biggest (cons 0 '())) 0)
  (check-expect (biggest (cons 1 (cons 3 (cons 2 '())))) 3)
  (define (biggest l)
    (cond
      [(empty? (rest l)) (first l)]
      [else
        (cond
         [(< (first l) (biggest (rest l)))
	     (biggest (rest l))]
	    [else (first l)])]))

What is the running time of this function?

Well, in the first case then its running time is 4. What if there are
two elements in the list? To figure out the running time, we need to
look at the inner `cond` expression. Either case could happen, right?
But the first case looks like it does more work, so lets assume that
case always happens. We have to do the first operation. Then we do the
`biggest` call, on a one element list. Then we do a comparison and
then the biggest call again, also on a one-element list.

Lets say there were 4 elements in the list. Then we are going to do
the biggest call on the 3 element list twice. But, EACH of those is going
to do the biggest call on the 2 element list, for a total of how many
calls to the biggest function on the 1 element list? What if we started with
10 elements in the list? Then we'd do the 9 call twice, the 8 call 4
times, the 7 call 8 times, 6 call 16 times 5 -> 32, 4 -> 64, 3 -> 128.
Do you see where this is going?

This is an exponential time function!  O(2^n)

We can actually verify this empirically too:

(time (biggest (list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15)))
(time (biggest (list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16)))
(time (biggest (list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17)))
(time (biggest (list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18)))
(time (biggest (list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19)))
(time (biggest (list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20)))
(time (biggest (list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21)))
(time (biggest (list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22)))


Welcome to DrRacket, version 6.6.0.4--2016-08-23(2e5b4a65/d) [3m].
Language: Intermediate Student; memory limit: 1024 MB.
cpu time: 40 real time: 39 gc time: 0
15
cpu time: 80 real time: 81 gc time: 0
16
cpu time: 162 real time: 162 gc time: 0
17
cpu time: 468 real time: 471 gc time: 60
18
cpu time: 794 real time: 794 gc time: 45
19
cpu time: 1515 real time: 1531 gc time: 81
20
cpu time: 2880 real time: 2876 gc time: 163
21
cpu time: 5667 real time: 5655 gc time: 327
22
Both tests passed!
> 

And don't be confused: something taking "exponential time" means that
when you make the input one bigger, the time doubles (or triples or
quadruples, etc).
